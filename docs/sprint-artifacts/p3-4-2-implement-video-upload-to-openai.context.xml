<story-context id="p3-4-2-implement-video-upload-to-openai" v="2.0">
  <metadata>
    <epicId>P3-4</epicId>
    <storyId>P3-4.2</storyId>
    <title>Implement Video Upload to OpenAI</title>
    <status>ready-for-dev</status>
    <generatedAt>2025-12-07</generatedAt>
    <generator>BMAD Story Context Workflow</generator>
    <sourceStoryPath>docs/sprint-artifacts/p3-4-2-implement-video-upload-to-openai.md</sourceStoryPath>
    <version>2.0 - Updated with frame extraction approach based on new research</version>
  </metadata>

  <story>
    <asA>system</asA>
    <iWant>to send video clips to OpenAI GPT-4o</iWant>
    <soThat>users get video-native analysis from OpenAI</soThat>
    <tasks>
      <task id="1" ac="All" status="done">Research OpenAI video API capabilities - confirmed frame extraction approach</task>
      <task id="2" ac="4">Update PROVIDER_CAPABILITIES for OpenAI - set video: true, video_method: frame_extraction</task>
      <task id="3" ac="1,2">Implement describe_video() via frame extraction - use FrameExtractor + generate_multi_image_description()</task>
      <task id="4" ac="3">Add optional audio transcription support via Whisper API</task>
      <task id="5" ac="4">Update _try_video_native_analysis() to route OpenAI to frame-extraction pipeline</task>
      <task id="6" ac="5">Add token/cost tracking for video requests</task>
      <task id="7" ac="All">Write tests for describe_video, frame limits, audio transcription, routing</task>
    </tasks>
  </story>

  <acceptanceCriteria>
    <ac id="1">Given video clip, when video_native mode selected for OpenAI, then extracts frames at 1-4 fps, sends as images to GPT-4o, returns description</ac>
    <ac id="2">Given video exceeds practical limits, when analysis attempted, then limits to max 10 frames for cost control</ac>
    <ac id="3">Given video has audio, when analysis includes audio transcription (Whisper), then transcript included in prompt context</ac>
    <ac id="4">Given OpenAI configured as video-capable provider, when video_native triggered, then uses frame-extraction pipeline (not direct upload)</ac>
    <ac id="5">Given frame extraction and analysis completes, when response received, then token usage tracked accurately</ac>
  </acceptanceCriteria>

  <researchFindings>
    <finding priority="critical">
      <summary>OpenAI GPT-4o CAN process video - via frame extraction, NOT native upload</summary>
      <detail>GPT-4o API does not accept MP4 files directly. Official approach: extract frames (1-4 fps) + optionally extract audio (Whisper) + send frames as images with transcript as text.</detail>
      <sources>
        <source url="https://arxiv.org/abs/2410.21276">GPT-4o System Card - confirms video support in principle</source>
        <source url="https://community.openai.com/t/why-file-upload-does-not-accept-mp4-files/863541">OpenAI Community - MP4 not accepted</source>
        <source url="https://cookbook.openai.com/examples/gpt4o/introduction_to_gpt4o">OpenAI Cookbook - frame extraction approach</source>
        <source url="https://portkey.ai/docs/guides/integrations/introduction-to-gpt-4o">Portkey Docs - frame + audio approach</source>
      </sources>
    </finding>
    <finding priority="high">
      <summary>Existing infrastructure already supports this approach</summary>
      <detail>FrameExtractor (P3-2.1) and generate_multi_image_description() (P3-2.3) already exist. OpenAI describe_video() is essentially a wrapper that calls these.</detail>
    </finding>
    <finding priority="medium">
      <summary>Audio transcription via Whisper is optional enhancement</summary>
      <detail>Whisper API (whisper-1 or gpt-4o-transcribe) can transcribe audio. Include transcript in prompt for richer descriptions.</detail>
    </finding>
  </researchFindings>

  <artifacts>
    <docs>
      <doc>
        <path>docs/epics-phase3.md</path>
        <title>Phase 3 Epic Breakdown</title>
        <section>Epic P3-4: Native Video Analysis / Story P3-4.2</section>
        <snippet>Original story defined native video upload. Updated to frame extraction based on research.</snippet>
      </doc>
      <doc>
        <path>docs/sprint-artifacts/p3-4-1-add-provider-video-capability-detection.md</path>
        <title>Provider Capability Detection Story</title>
        <section>Implementation</section>
        <snippet>PROVIDER_CAPABILITIES structure, get_video_capable_providers(), supports_video() methods.</snippet>
      </doc>
    </docs>

    <code>
      <file>
        <path>backend/app/services/ai_service.py</path>
        <kind>provider-class</kind>
        <symbol>OpenAIProvider</symbol>
        <lines>299-468</lines>
        <reason>Target class for adding describe_video() method. Has generate_description() and generate_multi_image_description() patterns.</reason>
      </file>
      <file>
        <path>backend/app/services/ai_service.py</path>
        <kind>provider-method</kind>
        <symbol>OpenAIProvider.generate_multi_image_description</symbol>
        <lines>461-580</lines>
        <reason>Existing multi-image method to be called by describe_video() after frame extraction.</reason>
      </file>
      <file>
        <path>backend/app/services/ai_service.py</path>
        <kind>constant</kind>
        <symbol>PROVIDER_CAPABILITIES</symbol>
        <lines>94-123</lines>
        <reason>Must update OpenAI entry: video=true, video_method="frame_extraction", max_frames=10</reason>
      </file>
      <file>
        <path>backend/app/services/frame_extractor.py</path>
        <kind>service</kind>
        <symbol>FrameExtractor</symbol>
        <lines>37-100</lines>
        <reason>Existing service for extracting frames from video. Use extract_frames() with frame_count=10, strategy="evenly_spaced", filter_blur=True.</reason>
      </file>
      <file>
        <path>backend/app/services/protect_event_handler.py</path>
        <kind>handler</kind>
        <symbol>_try_video_native_analysis</symbol>
        <lines>1014-1114</lines>
        <reason>Must update to check video_method and route OpenAI to frame-extraction pipeline.</reason>
      </file>
      <file>
        <path>backend/tests/test_services/test_ai_service.py</path>
        <kind>test</kind>
        <symbol>AI Service tests</symbol>
        <lines>1-150</lines>
        <reason>Add tests for describe_video(), frame limits, audio transcription.</reason>
      </file>
    </code>

    <dependencies>
      <python>
        <package name="openai" version=">=1.54.0">OpenAI API client - includes Whisper for audio transcription</package>
        <package name="av" version=">=12.0.0">PyAV for frame extraction (existing) and audio extraction (new)</package>
        <package name="pillow" version=">=10.0.0">Image processing for frame encoding</package>
        <package name="opencv-python" version=">=4.12.0">Used by FrameExtractor for blur detection</package>
        <package name="pytest" version="7.4.3">Testing framework</package>
        <package name="pytest-asyncio" version="0.21.1">Async test support</package>
      </python>
    </dependencies>
  </artifacts>

  <constraints>
    <constraint type="architecture">OpenAI video_method is "frame_extraction" - NOT native upload. Use existing FrameExtractor + generate_multi_image_description().</constraint>
    <constraint type="architecture">Max 10 frames for cost control. FrameExtractor already supports this limit (FRAME_EXTRACT_MAX_COUNT=10).</constraint>
    <constraint type="architecture">Audio transcription via Whisper is OPTIONAL. Make it a configurable setting.</constraint>
    <constraint type="coding">Reuse existing infrastructure: don't duplicate frame extraction or multi-image logic.</constraint>
    <constraint type="coding">describe_video() should call generate_multi_image_description() internally.</constraint>
    <constraint type="testing">Mock OpenAI client and FrameExtractor in tests.</constraint>
    <constraint type="testing">Test both with and without audio transcription.</constraint>
  </constraints>

  <interfaces>
    <interface>
      <name>OpenAIProvider.describe_video</name>
      <kind>async method (NEW)</kind>
      <signature>async def describe_video(self, video_path: Path, camera_name: str, timestamp: str, detected_objects: List[str], include_audio: bool = False, custom_prompt: Optional[str] = None) -> AIResult</signature>
      <path>backend/app/services/ai_service.py</path>
    </interface>
    <interface>
      <name>OpenAIProvider._transcribe_audio</name>
      <kind>async method (NEW)</kind>
      <signature>async def _transcribe_audio(self, video_path: Path) -> Optional[str]</signature>
      <path>backend/app/services/ai_service.py</path>
    </interface>
    <interface>
      <name>FrameExtractor.extract_frames</name>
      <kind>async method (EXISTING)</kind>
      <signature>async def extract_frames(self, clip_path: Path, frame_count: int = 5, strategy: str = "evenly_spaced", filter_blur: bool = True) -> List[bytes]</signature>
      <path>backend/app/services/frame_extractor.py</path>
    </interface>
    <interface>
      <name>OpenAIProvider.generate_multi_image_description</name>
      <kind>async method (EXISTING)</kind>
      <signature>async def generate_multi_image_description(self, images_base64: List[str], camera_name: str, timestamp: str, detected_objects: List[str], custom_prompt: Optional[str] = None) -> AIResult</signature>
      <path>backend/app/services/ai_service.py:461</path>
    </interface>
    <interface>
      <name>openai.audio.transcriptions.create</name>
      <kind>API method</kind>
      <signature>client.audio.transcriptions.create(model="whisper-1", file=audio_file) -> Transcription</signature>
      <path>openai library</path>
    </interface>
  </interfaces>

  <referenceImplementation>
    <description>OpenAI Cookbook pattern for video analysis via frame extraction</description>
    <frameExtraction>
      <comment>Extract 1 frame/sec using ffmpeg (we use PyAV FrameExtractor)</comment>
      <code>ffmpeg.input(VIDEO_PATH).filter('fps', fps=1).output(f"{FRAMES_DIR}/frame_%04d.jpg").run()</code>
    </frameExtraction>
    <audioTranscription>
      <comment>Optional: Extract and transcribe audio via Whisper API</comment>
      <code>
audio_transcript = client.audio.transcriptions.create(
    model="gpt-4o-transcribe",  # or "whisper-1"
    file=open(AUDIO_PATH, "rb")
).text
      </code>
    </audioTranscription>
    <apiCall>
      <comment>Send frames + transcript to GPT-4o</comment>
      <code>
messages = [
    {"role": "system", "content": "You are a video analysis assistant."},
    {"role": "user", "content": [
        {"type": "text", "text": "Here are frames extracted from a video. Analyze what's happening."},
        {"type": "text", "text": f"Audio transcript: {audio_transcript if audio_transcript else 'No audio'}"},
    ]},
]

# Add images to the message
for img in images:
    messages[1]["content"].append({
        "type": "image_url",
        "image_url": {"url": f"data:image/jpeg;base64,{img}"}
    })

response = client.chat.completions.create(model="gpt-4o", messages=messages)
      </code>
    </apiCall>
    <ourImplementation>
      <comment>Our describe_video() leverages existing services</comment>
      <code>
async def describe_video(self, video_path: Path, camera_name: str, timestamp: str,
                         detected_objects: List[str], include_audio: bool = False,
                         custom_prompt: Optional[str] = None) -> AIResult:
    from app.services.frame_extractor import get_frame_extractor

    # 1. Extract frames using existing FrameExtractor (P3-2.1)
    frame_extractor = get_frame_extractor()
    frames = await frame_extractor.extract_frames(
        clip_path=video_path, frame_count=10,
        strategy="evenly_spaced", filter_blur=True
    )

    if not frames:
        return AIResult(success=False, error="No frames extracted from video")

    frames_base64 = [self._frame_to_base64(frame) for frame in frames]

    # 2. Optional: Transcribe audio via Whisper
    transcript = await self._transcribe_audio(video_path) if include_audio else None

    # 3. Build enhanced prompt with transcript context
    enhanced_prompt = f"Audio transcript: {transcript}\n\n{custom_prompt or ''}" if transcript else custom_prompt

    # 4. Use existing multi-image method (P3-2.3)
    return await self.generate_multi_image_description(
        images_base64=frames_base64, camera_name=camera_name,
        timestamp=timestamp, detected_objects=detected_objects,
        custom_prompt=enhanced_prompt
    )
      </code>
    </ourImplementation>
  </referenceImplementation>

  <tests>
    <standards>Backend uses pytest with pytest-asyncio for async tests. Mock external APIs using unittest.mock (patch, AsyncMock). Use fixture test_db for database tests.</standards>
    <locations>
      <location>backend/tests/test_services/test_ai_service.py</location>
      <location>backend/tests/test_services/test_fallback_chain.py</location>
    </locations>
    <ideas>
      <idea ac="1">Test describe_video() calls FrameExtractor and generate_multi_image_description()</idea>
      <idea ac="2">Test frame count limited to max 10 even for long videos</idea>
      <idea ac="3">Test audio transcription included in prompt when enabled</idea>
      <idea ac="3">Test graceful handling when video has no audio track</idea>
      <idea ac="4">Test PROVIDER_CAPABILITIES has video_method="frame_extraction"</idea>
      <idea ac="4">Test _try_video_native_analysis routes OpenAI correctly</idea>
      <idea ac="5">Test token usage tracked from multi-image response</idea>
    </ideas>
  </tests>
</story-context>
