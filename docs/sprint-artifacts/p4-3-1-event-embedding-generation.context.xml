<story-context id="p4-3-1-event-embedding-generation" v="1.0">
  <metadata>
    <epicId>P4-3</epicId>
    <storyId>P4-3.1</storyId>
    <title>Event Embedding Generation</title>
    <status>drafted</status>
    <generatedAt>2025-12-11</generatedAt>
    <generator>BMAD Story Context Workflow</generator>
    <sourceStoryPath>docs/sprint-artifacts/p4-3-1-event-embedding-generation.md</sourceStoryPath>
  </metadata>

  <story>
    <asA>home security system user</asA>
    <iWant>event thumbnails to be converted into vector embeddings using CLIP</iWant>
    <soThat>the system can find similar past events and recognize recurring visitors/vehicles</soThat>
    <tasks>
      <task id="1" acs="1,5">Create EmbeddingService with CLIP model
        <subtask>Add sentence-transformers to requirements.txt</subtask>
        <subtask>Create backend/app/services/embedding_service.py</subtask>
        <subtask>Implement model lazy loading on first use</subtask>
        <subtask>Add generate_embedding(image_bytes) method</subtask>
        <subtask>Add timing instrumentation for performance monitoring</subtask>
        <subtask>Handle CUDA/MPS availability for GPU acceleration</subtask>
      </task>
      <task id="2" acs="3,6,11">Create EventEmbedding database model
        <subtask>Create backend/app/models/event_embedding.py</subtask>
        <subtask>Add fields: id, event_id, embedding (Text/JSON), model_version, created_at</subtask>
        <subtask>Create relationship with Event model</subtask>
        <subtask>Add unique constraint on event_id</subtask>
        <subtask>Create Alembic migration for new table</subtask>
      </task>
      <task id="3" acs="2,7,10">Integrate embedding generation into event pipeline
        <subtask>Add embedding step to event_processor.py after AI description</subtask>
        <subtask>Extract thumbnail bytes from base64 or file path</subtask>
        <subtask>Call EmbeddingService.generate_embedding()</subtask>
        <subtask>Store embedding in database</subtask>
        <subtask>Handle failures gracefully (log warning, don't block event creation)</subtask>
      </task>
      <task id="4" acs="8,9">Create batch processing endpoint
        <subtask>Add POST /api/v1/context/embeddings/batch endpoint</subtask>
        <subtask>Query events without embeddings</subtask>
        <subtask>Limit batch size to 100 events per request</subtask>
        <subtask>Process embeddings with progress tracking</subtask>
        <subtask>Return count of processed/failed embeddings</subtask>
      </task>
      <task id="5" acs="12">Create embedding status endpoint
        <subtask>Add GET /api/v1/context/embeddings/{event_id} endpoint</subtask>
        <subtask>Return embedding metadata (exists, model_version, created_at)</subtask>
        <subtask>Return 404 if event not found</subtask>
      </task>
      <task id="6" acs="1,4,5,6,7">Write unit tests
        <subtask>Test model initialization</subtask>
        <subtask>Test embedding dimension (512)</subtask>
        <subtask>Test embedding generation timing (&lt;200ms)</subtask>
        <subtask>Test model version tracking</subtask>
        <subtask>Test graceful failure handling</subtask>
        <subtask>Mock CLIP model for fast CI tests</subtask>
      </task>
      <task id="7" acs="2,3,8,9,10,11">Write integration tests
        <subtask>Test event creation with embedding storage</subtask>
        <subtask>Test both thumbnail modes (base64, file path)</subtask>
        <subtask>Test batch processing endpoint</subtask>
        <subtask>Test SQLite JSON storage</subtask>
      </task>
    </tasks>
  </story>

  <acceptanceCriteria>
    <criterion id="1">CLIP ViT-B/32 model loaded and initialized on startup</criterion>
    <criterion id="2">Embedding generated for each new event thumbnail</criterion>
    <criterion id="3">Embeddings stored in event_embeddings table with event_id reference</criterion>
    <criterion id="4">Embedding dimension is 512 (CLIP ViT-B/32 output)</criterion>
    <criterion id="5">Embedding generation completes in &lt;200ms per image</criterion>
    <criterion id="6">Model version tracked in database for future compatibility</criterion>
    <criterion id="7">Graceful fallback if embedding generation fails (event still created)</criterion>
    <criterion id="8">Batch processing endpoint for generating embeddings on existing events</criterion>
    <criterion id="9">Batch processing respects rate limiting (max 100 events per request)</criterion>
    <criterion id="10">Embedding generation works for both base64 and file-path thumbnails</criterion>
    <criterion id="11">SQLite fallback stores embeddings as JSON array (no pgvector required)</criterion>
    <criterion id="12">API endpoint to check embedding status for an event</criterion>
  </acceptanceCriteria>

  <artifacts>
    <docs>
      <doc>
        <path>docs/epics-phase4.md</path>
        <title>Phase 4 Epics</title>
        <section>Epic P4-3: Temporal Context Engine</section>
        <snippet>Story P4-3.1: Event Embedding Generation - Integrate CLIP or SigLIP model, generate embeddings for event thumbnails, store embeddings in database, implement batch processing for existing events.</snippet>
      </doc>
      <doc>
        <path>docs/PRD-phase4.md</path>
        <title>Phase 4 PRD</title>
        <section>FR1 - Temporal Context</section>
        <snippet>System stores event embeddings for similarity comparison. System identifies recurring visitors based on appearance similarity. Privacy-first: face embeddings stored locally only, never sent to cloud.</snippet>
      </doc>
      <doc>
        <path>docs/architecture.md</path>
        <title>Architecture</title>
        <section>Phase 4 Additions - ADR-P4-001</section>
        <snippet>Decision: Use CLIP ViT-B/32 for image embeddings. 512-dimensional embeddings, ~100ms inference time per image. Well-supported by sentence-transformers. SQLite + numpy fallback for MVP.</snippet>
      </doc>
      <doc>
        <path>docs/architecture.md</path>
        <title>Architecture</title>
        <section>Phase 4 Database Schema Additions</section>
        <snippet>event_embeddings table: id UUID, event_id UUID FK, embedding VECTOR(512), model_version TEXT, created_at TIMESTAMP. Index on embedding for vector similarity search.</snippet>
      </doc>
      <doc>
        <path>docs/architecture.md</path>
        <title>Architecture</title>
        <section>Phase 4 Technology Stack Additions</section>
        <snippet>sentence-transformers&gt;=2.2.0 for CLIP/SigLIP embeddings. pgvector&gt;=0.2.0 for PostgreSQL vector similarity search (optional).</snippet>
      </doc>
      <doc>
        <path>docs/sprint-artifacts/p4-2-5-camera-status-sensors.md</path>
        <title>Previous Story Reference</title>
        <section>Dev Agent Record</section>
        <snippet>Service singleton pattern via get_*_service() dependency. Event processor has Step 8 for MQTT publishing. Graceful degradation: log warnings but don't fail event creation.</snippet>
      </doc>
    </docs>

    <code>
      <file>
        <path>backend/app/services/event_processor.py</path>
        <kind>service</kind>
        <symbol>EventProcessor._process_event</symbol>
        <lines>541-868</lines>
        <reason>Main event processing pipeline - embedding generation will be added as Step 9 after MQTT publishing (Step 8). Contains thumbnail extraction logic at line 607.</reason>
      </file>
      <file>
        <path>backend/app/models/event.py</path>
        <kind>model</kind>
        <symbol>Event</symbol>
        <lines>1-107</lines>
        <reason>Event model with thumbnail_base64 and thumbnail_path fields. EventEmbedding will reference this via event_id FK.</reason>
      </file>
      <file>
        <path>backend/app/models/__init__.py</path>
        <kind>config</kind>
        <symbol>__all__</symbol>
        <lines>1-29</lines>
        <reason>Model exports - EventEmbedding must be added here after creation.</reason>
      </file>
      <file>
        <path>backend/app/services/mqtt_service.py</path>
        <kind>service</kind>
        <symbol>MQTTService, get_mqtt_service</symbol>
        <reason>Reference for service singleton pattern with get_*_service() dependency injection.</reason>
      </file>
      <file>
        <path>backend/app/services/mqtt_status_service.py</path>
        <kind>service</kind>
        <reason>Reference for service initialization in main.py lifespan context.</reason>
      </file>
      <file>
        <path>backend/tests/conftest.py</path>
        <kind>test</kind>
        <symbol>db_session, temp_db_file</symbol>
        <lines>1-60</lines>
        <reason>Test fixtures for database testing - use db_session for in-memory SQLite tests.</reason>
      </file>
      <file>
        <path>backend/tests/test_services/test_mqtt_status_sensors.py</path>
        <kind>test</kind>
        <reason>Reference for unit test patterns with mock services.</reason>
      </file>
      <file>
        <path>backend/tests/test_integration/test_mqtt_status_integration.py</path>
        <kind>test</kind>
        <reason>Reference for integration test patterns.</reason>
      </file>
    </code>

    <dependencies>
      <python>
        <package name="sentence-transformers" version="&gt;=2.2.0" note="NEW - Add to requirements.txt for CLIP model" />
        <package name="pillow" version="&gt;=10.0.0" note="Already present - for image processing" />
        <package name="sqlalchemy" version="&gt;=2.0.36" note="Already present - ORM" />
        <package name="alembic" version="&gt;=1.14.0" note="Already present - migrations" />
        <package name="fastapi" version="0.115.0" note="Already present - API framework" />
        <package name="pytest" version="7.4.3" note="Already present - testing" />
        <package name="pytest-asyncio" version="0.21.1" note="Already present - async testing" />
      </python>
      <node>
        <package name="@tanstack/react-query" version="^5.90.10" note="Server state management" />
      </node>
    </dependencies>
  </artifacts>

  <constraints>
    <constraint type="performance">Embedding generation must complete in &lt;200ms per image (ADR-P4-001)</constraint>
    <constraint type="architecture">Use lazy model loading - don't load CLIP model until first embedding request</constraint>
    <constraint type="architecture">Follow service singleton pattern with get_embedding_service() for dependency injection</constraint>
    <constraint type="architecture">Add embedding step to event_processor.py as Step 9 after MQTT publishing (Step 8)</constraint>
    <constraint type="privacy">Embeddings stored locally only - never sent to cloud (NFR1)</constraint>
    <constraint type="resilience">Embedding failures must not block event creation - log warning and continue (graceful degradation)</constraint>
    <constraint type="database">Use Text column with JSON array for SQLite compatibility (no pgvector required for MVP)</constraint>
    <constraint type="testing">Minimum 70% code coverage for new code (Definition of Done)</constraint>
    <constraint type="model">Use CLIP ViT-B/32 via sentence-transformers - 512-dimensional output</constraint>
    <constraint type="batch">Batch processing limited to 100 events per request for rate limiting</constraint>
  </constraints>

  <interfaces>
    <interface>
      <name>EmbeddingService.generate_embedding</name>
      <kind>async method</kind>
      <signature>async def generate_embedding(self, image_bytes: bytes) -> list[float]</signature>
      <path>backend/app/services/embedding_service.py (TO CREATE)</path>
    </interface>
    <interface>
      <name>EmbeddingService.store_embedding</name>
      <kind>async method</kind>
      <signature>async def store_embedding(self, event_id: str, embedding: list[float]) -> None</signature>
      <path>backend/app/services/embedding_service.py (TO CREATE)</path>
    </interface>
    <interface>
      <name>get_embedding_service</name>
      <kind>dependency</kind>
      <signature>def get_embedding_service() -> EmbeddingService</signature>
      <path>backend/app/services/embedding_service.py (TO CREATE)</path>
    </interface>
    <interface>
      <name>POST /api/v1/context/embeddings/batch</name>
      <kind>REST endpoint</kind>
      <signature>POST /api/v1/context/embeddings/batch -> {processed: int, failed: int, total: int}</signature>
      <path>backend/app/api/v1/context.py (TO CREATE)</path>
    </interface>
    <interface>
      <name>GET /api/v1/context/embeddings/{event_id}</name>
      <kind>REST endpoint</kind>
      <signature>GET /api/v1/context/embeddings/{event_id} -> {exists: bool, model_version: str, created_at: datetime}</signature>
      <path>backend/app/api/v1/context.py (TO CREATE)</path>
    </interface>
  </interfaces>

  <tests>
    <standards>
      Backend uses pytest with pytest-asyncio for async tests. Tests are organized by type: test_services/ for unit tests, test_api/ for API tests, test_integration/ for integration tests. Use db_session fixture from conftest.py for in-memory SQLite database testing. Mock external services (CLIP model) for fast CI tests. Target 70%+ coverage for new code per DoD.
    </standards>
    <locations>
      <location>backend/tests/test_services/test_embedding_service.py (TO CREATE)</location>
      <location>backend/tests/test_integration/test_embedding_integration.py (TO CREATE)</location>
      <location>backend/tests/test_api/test_context.py (TO CREATE)</location>
    </locations>
    <ideas>
      <idea ac="1">Test CLIP model lazy loading - verify model only loaded on first generate_embedding call</idea>
      <idea ac="4">Test embedding dimension - assert len(embedding) == 512</idea>
      <idea ac="5">Test embedding performance - measure time, assert &lt; 200ms</idea>
      <idea ac="6">Test model_version stored correctly in database</idea>
      <idea ac="7">Test graceful failure - corrupt image input should log warning, not raise exception</idea>
      <idea ac="2,3">Test event creation stores embedding in event_embeddings table</idea>
      <idea ac="10">Test both thumbnail modes - base64 string and file path</idea>
      <idea ac="11">Test SQLite JSON storage - embedding stored as valid JSON array</idea>
      <idea ac="8">Test batch endpoint processes events without embeddings</idea>
      <idea ac="9">Test batch endpoint enforces 100 event limit</idea>
      <idea ac="12">Test status endpoint returns correct metadata for existing embedding</idea>
      <idea ac="12">Test status endpoint returns 404 for non-existent event</idea>
    </ideas>
  </tests>
</story-context>
